{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<hr/>\n# **k-NN, Logistic Regression and k-Fold Cross Validation from Scratch**\n<span id=\"0\"></span>\n[**Burhan Y. Kiyakoglu**](https://www.kaggle.com/burhanykiyakoglu)\n<hr/>\n<font color=green>\n\n1. [Overview](#1)\n1. [Importing Modules, Reading the Dataset](#2)\n1. [k-Nearest Neighbors (k-NN)](#3)\n1. [Logistic Regression](#4)\n   * [Sigmoid Function](#5)\n   * [Cost Function](#6)\n   * [Gradient Descent Function](#7)\n   * [Main Logistic Function](#8)\n1. [Logistic Regression from Neural Network Perspective](#9)\n   * [Propagation](#10)\n   * [Optimization](#11)\n   * [Predict](#12)\n   * [Main Function](#13)\n1. [Testing the Functions](#14)\n   * [k-NN from Scratch](#15)\n   * [k-NN from Scratch vs scikit-learn k-NN](#16)\n   * [Logistic Regression from Scratch](#17)\n   * [Logistic Regression from Scratch vs Logistic Regression from Neural Network Perspective](#18)\n   * [Logistic Regression from Scratch vs scikit-learn Logistic Regression](#19)\n1. [k-Fold Cross Validation from Scratch](#20)   \n   * [k-Fold Cross Validation from Scratch vs scikit-learn k-Fold Cross Validation](#21) \n1. [Conclusion](#22)   ","metadata":{"_uuid":"0d22ced29cd84b5b93cc1d665d21fec2205a1738"}},{"cell_type":"markdown","source":"# <span id=\"1\"></span> Overview\n<hr/>\nWelcome to my Kernel! In this kernel I aim to apply machine learning algorithms by my own functions. By doing this, I belive that we will undestand the mechanism and theory behind the scence better.\n\nIf you have a question or feedback, feel free to write and if you like this kernel, please  leave an <font color=\"green\"><b>UPVOTE</b> </font>:  **It will be very much appreciated and will motivate me to offer more content to the** <font color=#47A8E5><b>kaggle</b> </font> **community** ðŸ™‚ \n<br/>\n<img src=\"https://i.imgur.com/QPWu3Rd.png\" title=\"source: Gradient Descent\" height=\"400\" width=\"800\" />","metadata":{"_uuid":"d993248fa94658e370afc6b198e1d75bdc4d4400"}},{"cell_type":"markdown","source":"# <span id=\"2\"></span> Importing Modules, Reading the Dataset\n#### [Return Contents](#0)\n<hr/>","metadata":{"_uuid":"7d6f339a170ef25a12430765fa3df8849bab3f5a"}},{"cell_type":"markdown","source":"In order to make some analysis, we need to set our environment up. To do this, I firstly imported some modules and read the data. The below output is the head of the data but if you want to see more details, you might try removing ***#*** signs in front of the ***df.describe()*** and ***df.info()***. ","metadata":{"_uuid":"c671ea4d0e62928ee750fc9da37cd066781621f4"}},{"cell_type":"code","source":"#import libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom random import randrange\nfrom random import seed\nfrom statistics import mean \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Reading Data \ndf = pd.read_csv('../input/Iris.csv')\n#df.describe()\n#df.info()\ndf['Class']=df['Species']\ndf['Class'] = df['Class'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\ndf[\"Class\"].unique()\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"3\"></span> k-Nearest Neighbors (k-NN)\n#### [Return Contents](#0)\n<hr/>","metadata":{"_uuid":"72a1d566456db01f5349d988982dfc84104ed159"}},{"cell_type":"markdown","source":"At k-NN, we find the k nearest neighbors of a point and then count these neighbors' labels. Afterwards, this point gets the label that has the highest count. In order to write my main function easier, I defined the distance functions as by below definitions.\n\n$$\\textbf{Euclidian Distance}$$\n$$ $$\n$$d(i,j)=\\sqrt{\\sum_{k=1}^n (x_{i,k}-x_{j,k})^{2}}$$\n$$ $$\n$$\\textbf{Manhattan Distance}$$\n$$ $$\n$$d(i,j)=\\sum_{k=1}^n |x_{i,k}-x_{j,k}|\\;$$\n$$ $$\n$$\\textbf{Minkowski Distance}$$\n$$ $$\n$$d(i,j)=\\left(\\sum_{k=1}^n |x_{i,k}-x_{j,k}|^{q}\\right)^{1/q}$$","metadata":{"_uuid":"aebee485f52ef48acdf3490a5c85aa4750bd2fb0"}},{"cell_type":"code","source":"# Distances\ndef euclidian(p1, p2): \n    dist = 0\n    for i in range(len(p1)):\n        dist = dist + np.square(p1[i]-p2[i])\n    dist = np.sqrt(dist)\n    return dist;\n\ndef manhattan(p1, p2): \n    dist = 0\n    for i in range(len(p1)):\n        dist = dist + abs(p1[i]-p2[i])\n    return dist;\n\ndef minkowski(p1, p2, q): \n    dist = 0\n    for i in range(len(p1)):\n        dist = dist + abs(p1[i]-p2[i])**q\n    dist = np.sqrt(dist)**(1/q)\n    return dist;","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below code is my main function. It calculates the distance between a point and all points in the dataset. Then, it takes the k nearest points and count the labels. Finally, it returns the label that has the maximum count.","metadata":{"_uuid":"8d3c6c8f256c7bc2a422acb44bf4b00406f20632"}},{"cell_type":"code","source":"# kNN Function\ndef kNN(X_train,y_train, X_test, k, dist='euclidian',q=2):\n    pred = []\n    # Adjusting the data type\n    if isinstance(X_test, np.ndarray):\n        X_test=pd.DataFrame(X_test)\n    if isinstance(X_train, np.ndarray):\n        X_train=pd.DataFrame(X_train)\n        \n    for i in range(len(X_test)):    \n        # Calculating distances for our test point\n        newdist = np.zeros(len(y_train))\n\n        if dist=='euclidian':\n            for j in range(len(y_train)):\n                newdist[j] = euclidian(X_train.iloc[j,:], X_test.iloc[i,:])\n    \n        if dist=='manhattan':\n            for j in range(len(y_train)):\n                newdist[j] = manhattan(X_train.iloc[j,:], X_test.iloc[i,:])\n    \n        if dist=='minkowski':\n            for j in range(len(y_train)):\n                newdist[j] = minkowski(X_train.iloc[j,:], X_test.iloc[i,:],q)\n\n        # Merging actual labels with calculated distances\n        newdist = np.array([newdist, y_train])\n\n        ## Finding the closest k neighbors\n        # Sorting index\n        idx = np.argsort(newdist[0,:])\n\n        # Sorting the all newdist\n        newdist = newdist[:,idx]\n        #print(newdist)\n\n        # We should count neighbor labels and take the label which has max count\n        # Define a dictionary for the counts\n        c = {'0':0,'1':0,'2':0 }\n        # Update counts in the dictionary \n        for j in range(k):\n            c[str(int(newdist[1,j]))] = c[str(int(newdist[1,j]))] + 1\n\n        key_max = max(c.keys(), key=(lambda k: c[k]))\n        pred.append(int(key_max))\n        \n    return pred","metadata":{"_uuid":"a9e649c7d20636508775b896453ac262b9eb2211","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"4\"></span> Logistic Regression\n#### [Return Contents](#0)\n<hr/>","metadata":{"_uuid":"4e6c70790386d89657f08aa4a93b51fc43afba69"}},{"cell_type":"markdown","source":"<span id=\"5\"></span>In order to get results between 0 and 1, a function, which is called **sigmoid**, is used to transform our hypothesis function. It is defined as\n$$ $$\n$$h_{\\theta}(x) = g(\\theta^{T} x)$$ \n$$ $$\nwhere $h_{\\theta}(x)$ is the hypothesis function, $x$ is a single record and \n$$ $$\n$$g(z)=\\dfrac{1}{1+e^{-z}}$$\n$$ $$\nBy using $g(\\theta^{T} x)$, we obtain the probablity and if $h_{\\theta}(x) \\geq 0.5$, we get $y=1$; if $h_{\\theta}(x) < 0.5$, we get $y=0$. Further, when $z \\geq 0$, $g(z) \\geq 0.5$ is another detail. Thus, if the $\\theta^{T} x \\geq 0$, then $y=1$.\n \nBy the definition, I defined the below ***sigmoid*** function.<span id=\"5\"></span>","metadata":{"_uuid":"df7687589c203f380f490e4acfdf3b9d9316c9a8"}},{"cell_type":"code","source":"# Sigmoid Function \ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))","metadata":{"_uuid":"eb5258febde81840eb27631992e8ab9925c06ea4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can't use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function. That's why we need to define a different cost function for logistic regression. It is simply defined as\n$$ $$\n$$J(\\theta) = \\dfrac{1}{m} \\sum^{m}_{i=1}Cost(h_{\\theta}(x^{(i)}), y^{(i)})$$ \n$$ $$\nwhere \n$$ $$\n$$Cost(h_{\\theta}(x^{(i)}), y^{(i)})=-y^{(i)} \\; log(h_{\\theta}(x^{(i)}))-(1-y^{(i)}) \\; log(1-h_{\\theta}(x^{(i)}))$$\n$$ $$\nAs the sanity check, $J(\\theta)$ can be plotted or printed as a function of the number of iterations to be sure that $J(\\theta)$ is **decreasing on every iteration**, which shows that it is converging correctly. At this point, choice of $\\alpha$ is important. If we select a high or small $\\alpha$ value, we might have problem about the converging.<span id=\"6\"></span>","metadata":{"_uuid":"84ba627d0aa6d11ad63dbfa9726e5b1d0959925e"}},{"cell_type":"code","source":"# Cost Function\ndef J(h, y):\n    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()","metadata":{"_uuid":"da4575d620133499cce857d4b48e2cc6a1c6ac39","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to find the $\\theta$ values that minimizes the cost function, I use gradient descent and we can summarize it as\n\nRepeat{\n       1. Calculate gradient average\n       2. Multiply by learning rate alpha\n       3. Subtract from theta\n}\n\nAlso, it can be mathemathically demonstrated as\n$$ $$\n$$\\textbf{Repeat}\\{ \\; \\theta_{j}:= \\theta_{j}-\\alpha \\dfrac{\\partial}{\\partial \\theta_{j}}J(\\theta) \\; \\} \\;  where \\;  j \\in \\{0,1,2,...,n \\}$$\n$$ $$\n$$or$$\n$$ $$\n$$\\textbf{Repeat}\\{ \\; \\theta_{j}:= \\theta_{j}-\\dfrac{\\alpha}{m} \\sum^{m}_{i=1} (h_{\\theta}(x^{(i)})-y^{(i)}) \\; x_{j}^{(i)} \\; \\} \\;  where \\;  j \\in \\{0,1,2,...,n \\}$$\n$$ $$\nAlgorithm looks identitcal to linear regression but be aware that this time $h_{\\theta}(x^{(i)})$ function has a **different definition** and that's why, they are not the same.\n\nI would also like to explain **regularization**. Regularization is designed to address the problem of overfitting and undefitting. To start with the **overfitting**, it means high variance and it is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data. This function fits to training data well but might cause poor results for the test set. On the other hand, **underfitting** means low variance and a very simple model. This might also cause poor results too. In this situation, we need to adjust features manually or use some model selection algoritms which brings an extra workload. Conversely, when we apply regularization, all the features are kept and the model adjusts $\\theta_{j}$. This especially works when we have a lot of slightly useful features.\n\nWhen we add regularization, the new cost fucntion is\n$$ $$\n$$J(\\theta) = \\dfrac{1}{m} \\sum^{m}_{i=1}\\left[-y^{(i)} \\; log(h_{\\theta}(x^{(i)}))-(1-y^{(i)}) \\; log(1-h_{\\theta}(x^{(i)}))\\right]+\\dfrac{\\lambda}{2m}\\sum^{n}_{j=1}\\theta^{2}_{j}$$\n$$ $$\nAlso, the new gradient descent can be mathemathically demonstrated as \n$$ $$\n$\\textbf{Repeat}\\{$ $$  \\theta_{0}:= \\theta_{0}-\\dfrac{\\alpha}{m} \\sum^{m}_{i=1} (h_{\\theta}(x^{(i)})-y^{(i)}) \\; x_{0}^{(i)} \\\\ \\theta_{j}:= \\theta_{j}- \\alpha \\left[ \\left( \\dfrac{1}{m} \\sum^{m}_{i=1} (h_{\\theta}(x^{(i)})-y^{(i)}) \\; x_{j}^{(i)} \\right) + \\dfrac{\\lambda}{m} \\; \\theta_{j} \\right] \\;where\\;  j \\in \\{1,2,...,n \\} $$ $\\}$ <span id=\"7\"></span>","metadata":{"_uuid":"b20d0cf55b3441a6bce44b9d1652af8081fed242"}},{"cell_type":"code","source":"# Gradient Descent Function\ndef gradientdescent(X, y, lmd, alpha, num_iter, print_cost):\n\n    # select initial values zero\n    theta = np.zeros(X.shape[1])\n    \n    costs = []  \n    \n    for i in range(num_iter):\n        z = np.dot(X, theta)\n        h = sigmoid(z)\n        \n        # adding regularization \n        reg = lmd / y.size * theta\n        # first theta is intercept\n        # it is not regularized\n        reg[0] = 0\n        cost = J(h, y)\n        \n        gradient = np.dot(X.T, (h - y)) / y.size + reg\n        theta = theta - alpha * gradient\n    \n        if print_cost and i % 100 == 0: \n            print('Number of Iterations: ', i, 'Cost : ', cost, 'Theta: ', theta)\n        if i % 100 == 0:\n            costs.append(cost)\n      \n    return theta, costs","metadata":{"_uuid":"d30c78e17b5ba4f2f9be5819232b68bc81298fac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to calculate the probability easily, I defined the below function but it is not essential.","metadata":{"_uuid":"d126a9ce8b745f97488ccc8f2a75526d88301708"}},{"cell_type":"code","source":"# Predict Function \ndef predict(X_test, theta):\n    z = np.dot(X_test, theta)\n    return sigmoid(z)","metadata":{"_uuid":"039263c50250d411113613adb77ba7012ac68b5e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, I defined my main function for the logistic regression. However, there is one more point to explain. When we have more than two classes we  can't apply the method we use for the binary classification. At this point, I prefered to use one vs all (one vs rest) method. Mathematically, it can be demonstrated as\n$$ $$\n$$h_{\\theta}^{(i)}(x)=P(y=i \\;  | \\;  x;\\theta) \\;\\;\\;\\;\\; (i=1,2,...,n)$$ \n$$ $$\nwhere $n$ is the number of classes. After calculating the above equation, we pick the class $i$ that maximizes $h_{\\theta}^{(i)}(x)$ to decide the class.<span id=\"8\"></span>","metadata":{"_uuid":"1cdfd8fc37f4d0d30da19f7720c59f4829b6d344"}},{"cell_type":"code","source":"# Main Logistic Function\ndef logistic(X_train, y_train, X_test, lmd=0, alpha=0.1, num_iter=30000, print_cost = False):\n    # Adding intercept\n    intercept = np.ones((X_train.shape[0], 1))\n    X_train = np.concatenate((intercept, X_train), axis=1)\n    \n    intercept = np.ones((X_test.shape[0], 1))\n    X_test = np.concatenate((intercept, X_test), axis=1)\n\n    # one vs rest\n    u=set(y_train)\n    t=[]\n    allCosts=[]   \n    for c in u:\n        # set the labels to 0 and 1\n        ynew = np.array(y_train == c, dtype = int)\n        theta_onevsrest, costs_onevsrest = gradientdescent(X_train, ynew, lmd, alpha, num_iter, print_cost)\n        t.append(theta_onevsrest)\n        \n        # Save costs\n        allCosts.append(costs_onevsrest)\n        \n    # Calculate probabilties\n    pred_test = np.zeros((len(u),len(X_test)))\n    for i in range(len(u)):\n        pred_test[i,:] = predict(X_test,t[i])\n    \n    # Select max probability\n    prediction_test = np.argmax(pred_test, axis=0)\n    \n    # Calculate probabilties\n    pred_train = np.zeros((len(u),len(X_train)))\n    for i in range(len(u)):\n        pred_train[i,:] = predict(X_train,t[i])\n    \n    # Select max probability\n    prediction_train = np.argmax(pred_train, axis=0)\n    \n    d = {\"costs\": allCosts,\n         \"Y_prediction_test\": prediction_test, \n         \"Y_prediction_train\" : prediction_train, \n         \"learning_rate\" : alpha,\n         \"num_iterations\": num_iter,\n         \"lambda\": lmd}\n        \n    return d","metadata":{"_uuid":"d380d32ee8c5af6202c40b98c16a0b889141e7db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"9\"></span> Logistic Regression from Neural Network Perspective\n#### [Return Contents](#0)\n<hr/>","metadata":{}},{"cell_type":"markdown","source":"In the previous section I explaned logistic regression and created my functions but I also want to explain it with the neural network mindset. Althought, the below functions will do the same and are similar to the above functions, I think this section will help us to understand neural networks better for the further studies. Since I already explained most of the details in the previous section, I will not go into too much detail.\n\nI would like to start with the below computation graph which summarizes the neural network perspective\n$$ $$\n\\begin{split}\n\\large x &\\\\\n\\large w & \\;\\; \\large\\rightleftarrows \\; \\boxed{z = wx + b} \\; \\rightleftarrows \\; \\boxed{a = \\sigma(z)} \\; \\rightleftarrows \\; \\boxed{\\mathcal{L}(a,y)}\\\\\n\\large b &\n\\end{split}\n$$ $$\nwhere $\\sigma$ represents the sigmoid function, $\\mathcal{L}$ is the loss, $\\mathcal{L}(\\hat y, y)= -y \\; log(\\hat y)-(1-y) \\; log(1-\\hat y)$, the right arrows determine the forward propogation and the left arrows determine the backpropagation. \n\nAbove graph gives hint about the way we follow but we do not have a single $x$ or $w$ at logistic regression. Thus, the below mathematical algorithm might be more clear. For one example $x^{(i)}$\n\n$$z^{(i)} = w^T x^{(i)} + b $$\n$$ $$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$ \n$$ $$\n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n$$ $$\nThen, the cost is computed by summing over all training examples\n$$ $$\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$\n$$ $$\nFor the backpropagation we will use the below derivaitions (I did not determined the all derivation steps for the backward elemination)  \n$$ $$\n$$ \\partial w = \\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial w}  = \\frac{1}{m}X(A-Y)^T \\\\$$ \n$$ \\partial b = \\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$","metadata":{}},{"cell_type":"code","source":"# Sigmoid Function\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n# Select initial values zero\ndef initialize_with_zeros(dim):\n    return np.zeros((dim,1)), 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span id=\"10\"></span> Propagation","metadata":{}},{"cell_type":"markdown","source":"$$\\textbf{Forward Propagation}$$\n$$ $$\n\\begin{split}\nX \\;\\; & \\large \\Rightarrow & \\;\\; A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)}) \\;\\; & \\large \\Rightarrow & \\;\\; J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\n\\end{split}\n$$ $$\n$$\\textbf{Backpropagation}$$\n$$ $$\n$$ \\partial w = \\frac{1}{m}X(A-Y)^T $$\n$$ $$\n$$ \\partial b = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$","metadata":{}},{"cell_type":"code","source":"def propagate(w, b, X, Y):\n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    A = sigmoid(np.dot(w.T,X)+b) # compute activation\n    cost = -1/m*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)) # compute cost\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    dw = 1/m*np.dot(X,(A-Y).T)\n    db = 1/m*np.sum(A-Y)\n    \n    # keep grads in a dictionary \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span id=\"11\"></span> Optimization","metadata":{}},{"cell_type":"markdown","source":"The goal is to learn $w$ and $b$ by minimizing the cost function $J$. Recall the gradient decent, for a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.","metadata":{}},{"cell_type":"code","source":"def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):    \n    costs = []\n    \n    for i in range(num_iterations):\n        # Cost and gradient calculation\n        grads, cost = propagate(w, b, X, Y)\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule\n        w = w-learning_rate*dw\n        b = b-learning_rate*db \n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n            \n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    # Save pameters and gradients\n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span id=\"12\"></span> Predict","metadata":{}},{"cell_type":"markdown","source":"In order to calculate the probability easily, we need the below function but it is not essential as in the previous section.","metadata":{}},{"cell_type":"code","source":"def predict_nn(w, b, X):    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities\n    A = sigmoid(np.dot(w.T,X)+b)\n        \n    return A","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span id=\"13\"></span> Main Function","metadata":{}},{"cell_type":"markdown","source":"Now, we put together all the building blocks. Also, I did not forget to make necessary adjustments to add one vs rest method.   ","metadata":{}},{"cell_type":"code","source":"def model(X_train, Y_train, X_test, Y_test, num_iterations = 30000, learning_rate = 0.1, print_cost = False): \n    # pandas to numpy\n    X_train = X_train.values\n    Y_train = Y_train.values.reshape((1,Y_train.shape[0]))\n    X_test = X_test.values\n    Y_test = Y_test.values.reshape((1,Y_test.shape[0]))\n    \n    # take transpose of X\n    X_train = X_train.T\n    X_test = X_test.T\n    \n    # initialize parameters with zeros \n    w, b = initialize_with_zeros(X_train.shape[0])\n    \n    # one vs all\n    u = set(y_train)\n    param_w = []\n    param_b = []\n    allCosts = []\n    for c in u:\n        # set the labels to 0 and 1\n        ynew = np.array(y_train == c, dtype = int)\n        # Gradient descent \n        parameters, grads, costs = optimize(w, b, X_train, ynew, num_iterations, learning_rate, print_cost = print_cost)\n        \n        # Save costs\n        allCosts.append(costs)\n        \n        # Retrieve parameters w and b from dictionary \"parameters\"\n        param_w.append(parameters[\"w\"])\n        param_b.append(parameters[\"b\"])\n    \n    # Calculate probabilties\n    pred_test = np.zeros((len(u),X_test.shape[1]))\n    for i in range(len(u)):\n        pred_test[i,:] = predict_nn(param_w[i], param_b[i], X_test)\n    \n    # Select max probability\n    Y_prediction_test = np.argmax(pred_test, axis=0)\n    \n    # Calculate probabilties\n    pred_train = np.zeros((len(u),X_train.shape[1]))\n    for i in range(len(u)):\n        pred_train[i,:] = predict_nn(param_w[i], param_b[i], X_train)\n    \n    # Select max probability\n    Y_prediction_train = np.argmax(pred_train, axis=0)\n        \n    d = {\"costs\": allCosts,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"14\"></span> Testing the Functions\n#### [Return Contents](#0)\n<hr/>","metadata":{"_uuid":"711126997f38cc9e2f3a406eab1ec1c538e5d8f9"}},{"cell_type":"markdown","source":"To test my functions, I defined 3 different points which are very close to the existing 3 points and splitted the data as training and test sets. I expect that the predicted labels will be the same as the real points and my functions will give similar results to scikit learn's functions.","metadata":{"_uuid":"e5cb68f023badb9290b12581de700820d502489e"}},{"cell_type":"code","source":"# I chose data points close to the real data points X[15], X[66] and X[130]\ntest = np.array([[5.77,4.44,1.55,0.44],[5.66,3.01,4.55,1.55],[7.44, 2.88, 6.11, 1.99]])\nprint(\"TEST POINTS\\n\", test)\n\nall_X = df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\nall_y = df['Class']\n\n# split data as training and test\ndf=df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm','Class']]\ntrain_data,test_data = train_test_split(df,train_size = 0.8,random_state=2)\nX_train = train_data[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ny_train = train_data['Class']\nX_test = test_data[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ny_test = test_data['Class']\n\ndef transform(i):\n    if i == 0:\n        return 'Iris-setosa'\n    if i == 1:\n        return 'Iris-versicolor'\n    if i == 2:\n        return 'Iris-virginica'","metadata":{"_uuid":"837b40c43a27e21be52ad6c22a9a0884cda4e886","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before starting to predict test points' labels, I wanted to see the places of these points according to some features. Thus, I drew the below charts which will help us to guess new points' labels.","metadata":{"_uuid":"4bd52baf012b5448f76eafe952e17a21105a93f5"}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nt=np.unique(all_y)\n\nax1=plt.subplot(2, 2, 1)\nax1.set(xlabel='Sepal Length (cm)', ylabel='Sepal Width (cm)')\nplt.plot(df[df['Class']==t[0]].iloc[:,0], df[df['Class']==t[0]].iloc[:,1], 'o', color='y')\nplt.plot(df[df['Class']==t[1]].iloc[:,0], df[df['Class']==t[1]].iloc[:,1], 'o', color='r')\nplt.plot(df[df['Class']==t[2]].iloc[:,0], df[df['Class']==t[2]].iloc[:,1], 'o', color='b')\n# test datapoints\nplt.plot(test[0,0],test[0,1],'*',color=\"k\")\nplt.plot(test[1,0],test[1,1],'*',color=\"k\")\nplt.plot(test[2,0],test[2,1],'*',color=\"k\")\n\nax2=plt.subplot(2, 2, 2)\nax2.set(xlabel='Petal Length (cm)', ylabel='Petal Width (cm)')\nax2.yaxis.set_label_position(\"right\")\nax2.yaxis.tick_right()\nplt.plot(df[df['Class']==t[0]].iloc[:,2], df[df['Class']==t[0]].iloc[:,3], 'o', color='y')\nplt.plot(df[df['Class']==t[1]].iloc[:,2], df[df['Class']==t[1]].iloc[:,3], 'o', color='r')\nplt.plot(df[df['Class']==t[2]].iloc[:,2], df[df['Class']==t[2]].iloc[:,3], 'o', color='b')\n# test datapoints\nplt.plot(test[0,2],test[0,3],'*',color=\"k\")\nplt.plot(test[1,2],test[1,3],'*',color=\"k\")\nplt.plot(test[2,2],test[2,3],'*',color=\"k\")\n\nax3=plt.subplot(2, 2, 3)\nax3.set(xlabel='Sepal Length (cm)', ylabel='Petal Length (cm)')\nplt.plot(df[df['Class']==t[0]].iloc[:,0], df[df['Class']==t[0]].iloc[:,2], 'o', color='y')\nplt.plot(df[df['Class']==t[1]].iloc[:,0], df[df['Class']==t[1]].iloc[:,2], 'o', color='r')\nplt.plot(df[df['Class']==t[2]].iloc[:,0], df[df['Class']==t[2]].iloc[:,2], 'o', color='b')\n# test datapoints\nplt.plot(test[0,0],test[0,2],'*',color=\"k\")\nplt.plot(test[1,0],test[1,2],'*',color=\"k\")\nplt.plot(test[2,0],test[2,2],'*',color=\"k\")\n\nax4=plt.subplot(2, 2, 4)\nax4.set(xlabel='Sepal Width (cm)', ylabel='Petal Width (cm)')\nax4.yaxis.set_label_position(\"right\")\nax4.yaxis.tick_right()\nplt.plot(df[df['Class']==t[0]].iloc[:,1], df[df['Class']==t[0]].iloc[:,3], 'o', color='y')\nplt.plot(df[df['Class']==t[1]].iloc[:,1], df[df['Class']==t[1]].iloc[:,3], 'o', color='r')\nplt.plot(df[df['Class']==t[2]].iloc[:,1], df[df['Class']==t[2]].iloc[:,3], 'o', color='b')\n# test datapoints\nplt.plot(test[0,1],test[0,3],'*',color=\"k\")\nplt.plot(test[1,1],test[1,3],'*',color=\"k\")\nplt.plot(test[2,1],test[2,3],'*',color=\"k\");\n","metadata":{"_uuid":"47fa8c3fc6f7899925ccfda9b7278d6ba3b10109","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"15\"></span> k-NN from Scratch","metadata":{"_uuid":"ba0401ee6bfea9609a23ef1a2e0c6f2524c0ceb4"}},{"cell_type":"markdown","source":"The result of the k-NN from scratch function for my test points are below:","metadata":{"_uuid":"bc81b528240ddf9d36e8d94d0806c310162d9c8e"}},{"cell_type":"code","source":"# Predicting the classes of the test data by kNN \n# Decide k value\nk = 5\n# print results\nprint(\"k-NN (\"+str(k)+\"-nearest neighbors)\\n\")\nc = kNN(all_X,all_y,test,k)\nfor i in range(len(c)):\n    ct=set(map(transform,[c[i]]))\n    print(\"Test point: \"+str(test[i,:])+\"  Label: \"+str(c[i])+\" \"+str(ct))","metadata":{"_uuid":"0173431e74bf98d83dc01aa15813cb3dfc1ef11b","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"16\"></span> k-NN from Scratch vs scikit-learn k-NN","metadata":{"_uuid":"c52799498e7f859dd6793a3abe5e32e5a9905fb9"}},{"cell_type":"markdown","source":"In this section, I compare my kNN function with the scikit learn's k-NN function and determine the confusion matrixes for the both models. The results look same but by removing the *random_state* in the *train_test_split* function and chaging the *train_size* different results can be found.","metadata":{"_uuid":"ecd8c62dd725efe2dbaea49a44f47576e4c39a85"}},{"cell_type":"code","source":"# k-NN from scratch\nc=kNN(X_train,y_train,X_test,k)\ncm=confusion_matrix(y_test, c)\n\n# logistic regression - scikit learn\nsck = KNeighborsClassifier(n_neighbors = k).fit(X_train, y_train)\nsck_cm=confusion_matrix(y_test, sck.predict(X_test))\n\nplt.figure(figsize=(15,6))\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\n\nplt.subplot(1,2,1)\nplt.title(\"k-NN from Scratch\")\nsns.heatmap(cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(1,2,2)\nplt.title(\"k-NN - scikit learn\")\nsns.heatmap(sck_cm, annot = True, cmap=\"Greens\",cbar=False);","metadata":{"_uuid":"1b7e8e35bc7eeeab9f12b45617904b7c076f5200","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"17\"></span> Logistic Regression from Scratch","metadata":{"_uuid":"7a36022326b4520675a92789e720fd2cfb47fd26"}},{"cell_type":"markdown","source":"The result of the logistic regression from scratch function for my test points are below:","metadata":{"_uuid":"a549e3b7ee0841c5a87a948566e857db262b0d0a"}},{"cell_type":"code","source":"# Predicting the classes of the test data by Logistic Regression\nprint(\"Logistic Regression\\n\")\nc=logistic(X_train,y_train,test)\n# print results\nfor i in range(len(c['Y_prediction_test'])):\n    ct=set(map(transform,[c['Y_prediction_test'][i]]))\n    print(\"Test point: \"+str(test[i,:])+\"  Label: \"+str(c['Y_prediction_test'][i])+\" \"+str(ct))","metadata":{"_uuid":"f34fe597680bbc21ec01b562cb53db80c1df74f4","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"18\"></span> Logistic Regression from Scratch vs Logistic Regression from Neural Network Perspective","metadata":{}},{"cell_type":"markdown","source":"Below confusion matrices show that each model gives the same results when the parameters are selected the same.","metadata":{}},{"cell_type":"code","source":"# logistic regression from scratch\nstart=dt.datetime.now()\nc=logistic(X_train,y_train,X_test)\n# Print train/test Errors\nprint('Elapsed time of logistic regression from scratch: ',str(dt.datetime.now()-start))\nprint(\"train accuracy: {} %\".format(100 - np.mean(np.abs(c[\"Y_prediction_train\"] - y_train)) * 100))\nprint(\"test accuracy: {} %\".format(100 - np.mean(np.abs(c[\"Y_prediction_test\"] - y_test)) * 100))\n\n\n# Logistic Regression from Neural Network Perspective\nstart=dt.datetime.now()\nd = model(X_train, y_train, X_test, y_test)\nprint('\\nElapsed time of Logistic Regression from Neural Network Perspective: ',str(dt.datetime.now()-start))\nprint(\"train accuracy: {} %\".format(100 - np.mean(np.abs(d[\"Y_prediction_train\"] - y_train)) * 100))\nprint(\"test accuracy: {} %\".format(100 - np.mean(np.abs(d[\"Y_prediction_test\"] - y_test)) * 100))\n\n\ncm=confusion_matrix(y_test, c['Y_prediction_test'])\n\nplt.figure(figsize=(15,6))\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\n\nplt.subplot(1,2,1)\nplt.title(\"Logistic Regression from Scratch\")\nsns.heatmap(cm, annot = True, cmap=\"Greens\",cbar=False);\n\ncm=confusion_matrix(y_test, d['Y_prediction_test'].reshape(30,))\n\nplt.subplot(1,2,2)\nplt.title(\"Logistic Regression from Neural Network Perspective\")\nsns.heatmap(cm, annot = True, cmap=\"Greens\",cbar=False);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Moreover, below line charts determine the costs for different learning rates. These plots generally used for the sanity check. They look like same for each model. They look pretty good and the effect of learning rate can be observed clearly. Since I used one vs rest, for each learning rate I drew 3 cost lines. ","metadata":{}},{"cell_type":"code","source":"# Learning rates\nlr = [0.1, 0.01, 0.001]\n\nfor i in range(len(lr)):\n    # Run the model for different learning rates\n    c = logistic(X_train,y_train,X_test, alpha = lr[i])\n    \n    # Adjust results to plot\n    dfcost = pd.DataFrame(list(c['costs'])).transpose()\n    dfcost.columns = ['0 (Iris-setosa) vs rest','1 (Iris-versicolor) vs rest','2 (Iris-virginica) vs rest']\n    \n    # Plot the costs\n    if i==0 : f, axes = plt.subplots(1, 3,figsize=(24,4))\n    sns.lineplot(data = dfcost.iloc[:, :3], ax=axes[i])\n    sns.despine(right=True, offset=True)\n    axes[i].set(xlabel='Iterations (hundreds)', ylabel='Cost ' +'(Learning Rate: ' + str(lr[i]) + ')')\n    \nplt.suptitle(\"Logistic Regression from Scratch\\n\",fontsize=24);  \n\nfor i in range(len(lr)):\n    # Run the model for different learning rates\n    d = model(X_train, y_train, X_test, y_test, learning_rate = lr[i])\n    \n    # Adjust results to plot\n    dfcost = pd.DataFrame(list(d['costs'])).transpose()\n    dfcost.columns = ['0 (Iris-setosa) vs rest','1 (Iris-versicolor) vs rest','2 (Iris-virginica) vs rest']\n    \n    # Plot the costs\n    if i==0 : f, axes = plt.subplots(1, 3,figsize=(30,5))\n    sns.lineplot(data = dfcost.iloc[:, :3], ax=axes[i])\n    sns.despine(right=True, offset=True)\n    axes[i].set(xlabel='Iterations (hundreds)', ylabel='Cost ' +'(Learning Rate: ' + str(lr[i]) + ')')\n    \nplt.suptitle(\"Logistic Regression from Neural Network Perspective\\n\",fontsize=24);    ","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"19\"></span> Logistic Regression from Scratch vs scikit-learn Logistic Regression","metadata":{"_uuid":"c6c5de131fbf2821dcf87aaa49504b46f9cb28df"}},{"cell_type":"markdown","source":"This time, I compare my logistic function with the scikit learn's logistic function and determine the confusion matrixes for the both models. Also, I changed the regularization parameter ($\\lambda$) for both models and determined the confusion matrixes for them too. The results look similar but they are not the same as expected. By removing the *random_state* in the *train_test_split* function and chaging the *train_size* different results can be obtained. ","metadata":{"_uuid":"3a303cf377c458c672216ea6f24d742474c980b8"}},{"cell_type":"code","source":"# logistic regression from scratch\nc=logistic(X_train,y_train,X_test)\ncm=confusion_matrix(y_test, c['Y_prediction_test'])\n\n# logistic regression - scikit learn\nsck = LogisticRegression().fit(X_train, y_train)\nsck_cm=confusion_matrix(y_test, sck.predict(X_test))\n\n# logistic regression from scratch\nc_r=logistic(X_train,y_train,X_test,lmd=0.01)\ncm_r=confusion_matrix(y_test, c_r['Y_prediction_test'])\n\n# logistic regression - scikit learn\nsck_r = LogisticRegression(C=100).fit(X_train, y_train)\nsck_cm_r=confusion_matrix(y_test, sck_r.predict(X_test))\n\nplt.figure(figsize=(15,12))\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\n\nplt.subplot(2,2,1)\nplt.title(\"Logistic Regression from Scratch\")\nsns.heatmap(cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,2)\nplt.title(\"Logistic Regression - scikit learn\")\nsns.heatmap(sck_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,3)\nplt.title(\"Logistic Regression from Scratch ( $\\lambda$ = 0.01 )\")\nsns.heatmap(cm_r, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,4)\nplt.title(\"Logistic Regression ( $\\lambda$ = 0.01 / C = 100 ) - scikit learn\")\nsns.heatmap(sck_cm_r, annot = True, cmap=\"Greens\",cbar=False);","metadata":{"_uuid":"8fa5253ec128d19bca8e5a92169d0b41d8e62f36","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"20\"></span> k-Fold Cross Validation from Scratch\n#### [Return Contents](#0)\n<hr/>","metadata":{"_uuid":"88487d942b26ea4bfd9aa4c2c04181f4657f200c"}},{"cell_type":"markdown","source":"k-Fold Cross Validation is a very useful technique to check how well a model performs when we apply it on an independent data. It is often used to flag problems caused by overfitting and selection bias. However, it brings an additional data processing load and time. \n\nThe below figure depicts the k-fold cross validation. Briefly, we randomly divide data to k folds, take one of the folds as the testing set in each step and calculate the accuracy. \n\n<img src=\"https://i.imgur.com/hq45Jfq.png\" title=\"source: imgur.com\" />\n\nI divided my k-fold cross validation to two parts. First, I defined the below function to split the data to k folds.","metadata":{"_uuid":"eb677c326b6723391df9cc7f5c6d72332a84479e"}},{"cell_type":"code","source":"def cross_validation_split(dataset, folds):\n        dataset_split = []\n        df_copy = dataset\n        fold_size = int(df_copy.shape[0] / folds)\n        \n        # for loop to save each fold\n        for i in range(folds):\n            fold = []\n            # while loop to add elements to the folds\n            while len(fold) < fold_size:\n                # select a random element\n                r = randrange(df_copy.shape[0])\n                # determine the index of this element \n                index = df_copy.index[r]\n                # save the randomly selected line \n                fold.append(df_copy.loc[index].values.tolist())\n                # delete the randomly selected line from\n                # dataframe not to select again\n                df_copy = df_copy.drop(index)\n            # save the fold     \n            dataset_split.append(np.asarray(fold))\n            \n        return dataset_split ","metadata":{"_uuid":"c271ce588290f16e45cf95f8b26df547e67d2769","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By using the *cross_validation_split* function I defined my main function below. This function takes each fold as test and returns the accuricies for each fold.","metadata":{"_uuid":"149fd6cd348f631ce96716c399ff89142f0b4dde"}},{"cell_type":"code","source":"def kfoldCV(dataset, f=5, k=5, model=\"logistic\"):\n    data=cross_validation_split(dataset,f)\n    result=[]\n    # determine training and test sets \n    for i in range(f):\n        r = list(range(f))\n        r.pop(i)\n        for j in r :\n            if j == r[0]:\n                cv = data[j]\n            else:    \n                cv=np.concatenate((cv,data[j]), axis=0)\n        \n        # apply the selected model\n        # default is logistic regression\n        if model == \"logistic\":\n            # default: alpha=0.1, num_iter=30000\n            # if you change alpha or num_iter, adjust the below line         \n            c = logistic(cv[:,0:4],cv[:,4],data[i][:,0:4])\n            test = c['Y_prediction_test']\n        elif model == \"knn\":\n            test = kNN(cv[:,0:4],cv[:,4],data[i][:,0:4],k)\n            \n        # calculate accuracy    \n        acc=(test == data[i][:,4]).sum()\n        result.append(acc/len(test))\n        \n    return result","metadata":{"_uuid":"6568837ff2c063e162ac53f13960f126f1468dec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe the accuricies of 3-fold cross validation for my logistic regression and k-NN from scratch functions below. In order to get different results, we need to comment out *seed(1)*. ","metadata":{"_uuid":"461d6c78f7c9e43f7e10443c49fdf470fe4c13a2"}},{"cell_type":"code","source":"print(\"3-Fold Cross Validation for Logistic Regression from Scratch\")\nprint(\"Fold Size:\",int(df.shape[0] / 3))\nseed(1)\nacc=kfoldCV(df,3)\nprint(\"Accuricies:\", acc)\nprint(\"Average of the Accuracy:\", round(mean(acc),2))\n\nprint(\"\\n3-Fold Cross Validation for k-NN from Scratch\")\nprint(\"Fold Size:\",int(df.shape[0] / 3))\nseed(1)\nacc=kfoldCV(df,3,model=\"knn\")\nprint(\"Accuricies:\", acc)\nprint(\"Average of the Accuracy:\", round(mean(acc), 2))","metadata":{"_uuid":"76ddc70b5f30f6dbc48f70db0947d42748194ca4","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the **bias-variance behavior**, higher training set means higher variance. At this point, depending to our choice of number of folds, accuracies might change. This change can be observed from the below figures for my models.\n","metadata":{"_uuid":"e62f1b10e401e16f6133c71d1c4470891acabfaa"}},{"cell_type":"code","source":"seed(1)\nbva_lr=[]\nbva_knn=[]\nfor f in range(2,11):\n    # k-fold cv from scratch for logistic regression\n    bva_lr.append(mean(kfoldCV(df,f)))\n    # k-fold cv from scratch for k-NN\n    bva_knn.append(mean(kfoldCV(df,f,model=\"knn\")))\n\n# plot the change in the average accuracy according to k \nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nplt.title(\"Logistic Regression\")\nplt.xlabel(\"Number of Folds (k)\")\nplt.ylabel(\"Average Accuracy\")\nplt.plot(range(2,11),bva_lr);\n\nplt.subplot(1,2,2)\nplt.title(\"k-NN\")\nplt.xlabel(\"Number of Folds (k)\")\nplt.ylabel(\"Average Accuracy\")\nplt.plot(range(2,11),bva_knn);","metadata":{"_uuid":"bd7abe5bb2c57750c4ee38e67e2b9ec724c06f41","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"21\"></span> k-Fold Cross Validation from Scratch vs scikit-learn k-Fold Cross Validation","metadata":{"_uuid":"7c890c1f5577e0b7c1ba3d63807d8a8971ea3b83","trusted":true}},{"cell_type":"markdown","source":"I compare my k-fold cross validation function with the scikit learn's k-fold cross validation function. Here, we shouldn't forget that my fuction uses the k-NN and logistic regression functions from scratch and this may increase the diffrence between the results. When I use *seed(1)* as in the previous sections, it gives plausible results. ","metadata":{"_uuid":"b1b0084c721cfe33ba0bb63f1af4793848ae3cc2"}},{"cell_type":"code","source":"seed(1)\nlr_scratch=kfoldCV(df,3)\nknn_scratch=kfoldCV(df,3,model=\"knn\")\nlr_sck=cross_val_score(LogisticRegression(), all_X, all_y, cv=3)\nknn_sck=cross_val_score(KNeighborsClassifier(n_neighbors = k), all_X, all_y, cv=3)\n\nprint(\"RESULTS\")\nprint(\"Logistic Regression & k-Fold Cross Validation from Scratch: \",lr_scratch,\"\\nMean: \",round(mean(lr_scratch),2))\nprint(\"\\nLogistic Regression & k-Fold Cross Validation (scikit-learn): \",lr_sck,\"\\nMean: \",round(mean(lr_sck),2))\nprint(\"\\nk-NN & k-Fold Cross Validation from Scratch: \",knn_scratch,\"\\nMean: \",round(mean(knn_scratch),2))\nprint(\"\\nk-NN & k-Fold Cross Validation (scikit-learn): \",knn_sck,\"\\nMean: \",round(mean(knn_sck),2))","metadata":{"_uuid":"7d4edb6d72f130121cb58234e607f2bbda27e88d","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span id=\"22\"></span> Conclusion\n#### [Return Contents](#0)\n<hr/>","metadata":{"_uuid":"7f349e7184fbe79b8acfcd5bbe32a5c7ab563ddb"}},{"cell_type":"markdown","source":"In this kernel, I used my own functions and tried to explain the theory behind them without using scikit-learn or any other built in functions. Probably, you use the bult in functions in your daily tasks as me and I guess they perform better. However, I thing digging deeper and understanding the logic behind them will make it easier to see the whole picture.  \n\n<b><font color=\"green\">Thank you for reading my kernel </font></b> **and If you liked this kernel, please** <b><font color=\"red\">do not forget to <b></font><font color=\"green\">UPVOTE </font></b> ðŸ™‚\n    \nIf you would like to glance my other notebooks, please [**CLICK HERE**](https://www.kaggle.com/burhanykiyakoglu/notebooks).     ","metadata":{"_uuid":"e5e5b459305f288e14e685efe84b2bdaf3bd670c"}}]}